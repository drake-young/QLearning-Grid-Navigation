# QLearning-Grid-Navigation
Employ the Use of Q-Learning to discover how to Navigate a Maze of Goal, Hole, and Wall States

In the process of learning about Q-Learning for my "AI with Reinforcement Learning" course, we were provided a sample notebook to perform a simple Q-learning task to navigate a hard-coded grid to find a Goal state and avoid the Hole state. I have since modified the code to operate more generically, allowing the user to adjust the dimensions of the grid and relocate the Goal, Hole, and Starting states. I have also added an additional type of state called a wall, which makes it impossible for the program to navigate into that state. The program runs successfully on a variety of test cases, and it was found that printing an episode generally took longer than training itself. Though, I have only considered a maximum grid area of about 100 cells. The scalability of this algorithm is yet to be determined and can be highly dependent on random chance, since the duration of an episode is not limited, allowing the training to potentially get caught in a repeating cycle without discovering a terminal state. 

Future considerations include limiting the duration of an episode so that cycles will not cause the program to hang indefinitely. Additionally, I would like to introduce a version that randomizes the starting state during training to improve the breadth of learning. Finally I would like to introduce a type of state that provides a custom reward amount (positive or negative) without being a terminal state (e.g. a "reward token" that is picked up in passing).
